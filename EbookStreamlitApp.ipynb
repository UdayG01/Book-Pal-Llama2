{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOPyzPCG3oX0X7mcdX76RI4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UdayG01/Book-Pal-Llama2/blob/main/EbookStreamlitApp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* In this notebook, I'll be creating a UI using streamlit for the Ebook guide developed using Llama2"
      ],
      "metadata": {
        "id": "7j7mrnilCYT2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Steps to run:\n",
        "1. Run all the 'pip install' blocks to install the necessary libraries.\n",
        "2. Run the block making the 'htmlTemplates.py' file.\n",
        "3. Run the block making the 'app.py' file.\n",
        "4. Run the 'streamlit run' and the 'npx localtunnel' blocks. You'll get a url of a new window, open that window in a new tab.\n",
        "5. After the statements 1-4, in the /content directory of google colab of the current project, you'll get a 'logs.txt' file made - copy the ip address of 'External URL' (don't copy the port number with the ip).\n",
        "6. Paste the ip in the window you opened in new tab and hit Submit.\n",
        "7. Upload the pdf you want to talk to in the Streamlit app, hit process and start querying."
      ],
      "metadata": {
        "id": "qrEHoT2TCdM0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ! pip install llama-cpp-python\n",
        "! pip install langchain\n",
        "! pip install pypdf\n",
        "! pip install unstructured\n",
        "! pip install sentence_transformers\n",
        "! pip install pinecone-client\n",
        "! pip install huggingface_hub\n",
        "! pip install chromadb\n",
        "\n",
        "! CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install --upgrade --force-reinstall llama-cpp-python --no-cache-dir\n",
        "# !CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python --force-reinstall --upgrade --no-cache-dir --verbose"
      ],
      "metadata": {
        "id": "KrVFj_D9Bm-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install streamlit\n",
        "! pip install pypdf2\n",
        "! npm install localtunnel"
      ],
      "metadata": {
        "id": "Flzjxt187NN2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile htmlTemplates.py\n",
        "\n",
        "\n",
        "css = '''\n",
        "<style>\n",
        ".chat-message {\n",
        "    padding: 1.5rem; border-radius: 0.5rem; margin-bottom: 1rem; display: flex\n",
        "}\n",
        ".chat-message.user {\n",
        "    background-color: #2b313e\n",
        "}\n",
        ".chat-message.bot {\n",
        "    background-color: #475063\n",
        "}\n",
        ".chat-message .avatar {\n",
        "  width: 20%;\n",
        "}\n",
        ".chat-message .avatar img {\n",
        "  max-width: 78px;\n",
        "  max-height: 78px;\n",
        "  border-radius: 50%;\n",
        "  object-fit: cover;\n",
        "}\n",
        ".chat-message .message {\n",
        "  width: 80%;\n",
        "  padding: 0 1.5rem;\n",
        "  color: #fff;\n",
        "}\n",
        "'''\n",
        "\n",
        "bot_template = '''\n",
        "<div class=\"chat-message bot\">\n",
        "    <div class=\"avatar\">\n",
        "        Llama\n",
        "    </div>\n",
        "    <div class=\"message\">{{MSG}}</div>\n",
        "</div>\n",
        "'''\n",
        "\n",
        "user_template = '''\n",
        "<div class=\"chat-message user\">\n",
        "    <div class=\"avatar\">\n",
        "        You\n",
        "    </div>\n",
        "    <div class=\"message\">{{MSG}}</div>\n",
        "</div>\n",
        "'''\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYPvj2qAB-N7",
        "outputId": "ea70db3f-6e73-4ac7-fb84-bdbd1d138c1a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing htmlTemplates.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import streamlit as st\n",
        "from PyPDF2 import PdfReader\n",
        "from langchain.document_loaders import PyPDFLoader, OnlinePDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "# from langchain.vectorstores import Pinecone\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "# import pinecone\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "\n",
        "# importing the files for the llm\n",
        "from langchain.llms import LlamaCpp\n",
        "from langchain.callbacks.manager import CallbackManager\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.llms import HuggingFaceHub\n",
        "from htmlTemplates import css, bot_template, user_template\n",
        "\n",
        "import os\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\"\n",
        "\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "model_name_or_path = \"TheBloke/Llama-2-13B-chat-GGML\"\n",
        "model_basename = \"llama-2-13b-chat.ggmlv3.q5_1.bin\"\n",
        "model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)\n",
        "\n",
        "def get_pdf_text(pdf_docs):\n",
        "  text = \"\"\n",
        "  for pdf in pdf_docs:\n",
        "    pdf_reader = PdfReader(pdf)\n",
        "    # loader = PyPDFLoader(pdf)\n",
        "    # data = loader.load()\n",
        "    for page in pdf_reader.pages:\n",
        "      # text += document.page_content\n",
        "      text += page.extract_text()\n",
        "  return text\n",
        "\n",
        "\n",
        "def get_text_chunks(text):\n",
        "   text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
        "  # text_splitter = CharacterTextSplitter(\n",
        "  #     chunk_size = 20000,\n",
        "  #     chunk_overlap = 100,\n",
        "  #     length_function = len\n",
        "  # )\n",
        "   docs = text_splitter.split_text(text)\n",
        "   return docs\n",
        "\n",
        "def get_vectorstore(docs):\n",
        "  embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
        "  db = Chroma.from_texts(docs, embeddings)\n",
        "  return db\n",
        "\n",
        "def get_conversation_chain(vectorstore):\n",
        "  n_gpu_layers = 40\n",
        "  n_batch = 256\n",
        "\n",
        "  # Loading model,\n",
        "  llm = LlamaCpp(\n",
        "      model_path=model_path,\n",
        "      max_tokens=256,\n",
        "      n_gpu_layers=n_gpu_layers,\n",
        "      n_batch=n_batch,\n",
        "      n_ctx=4096,\n",
        "      verbose=False,\n",
        "  )\n",
        "\n",
        "  # repo_id = \"google/flan-t5-xxl\"\n",
        "  # llm = HuggingFaceHub(repo_id=repo_id, model_kwargs={\"temperature\": 0.5, \"max_length\": 64})\n",
        "\n",
        "  memory = ConversationBufferMemory(memory_key = 'chat_history', return_messages=True)\n",
        "  conversation_chain = ConversationalRetrievalChain.from_llm(\n",
        "      llm=llm,\n",
        "      retriever=vectorstore.as_retriever(),\n",
        "      memory=memory\n",
        "  )\n",
        "  return conversation_chain\n",
        "\n",
        "def handle_userinput(user_question):\n",
        "  response = st.session_state.conversation({'question': user_question})\n",
        "  st.session_state.chat_history = response['chat_history']\n",
        "\n",
        "  for i, message in enumerate(st.session_state.chat_history):\n",
        "        if i % 2 == 0:\n",
        "            st.write(user_template.replace(\n",
        "                \"{{MSG}}\", message.content), unsafe_allow_html=True)\n",
        "        else:\n",
        "            st.write(bot_template.replace(\n",
        "                \"{{MSG}}\", message.content), unsafe_allow_html=True)\n",
        "\n",
        "\n",
        "def main():\n",
        "  st.set_page_config(page_title=\"Omnipresent.ai\",\n",
        "                    page_icon=\":alien:\")\n",
        "  st.write(css, unsafe_allow_html=True)\n",
        "\n",
        "  if \"conversation\" not in st.session_state:\n",
        "    st.session_state.conversation = None\n",
        "  if \"chat_history\" not in st.session_state:\n",
        "    st.session_state.chat_history = None\n",
        "\n",
        "\n",
        "  st.header(\"Have a word with your book :scroll:\")\n",
        "  user_question = st.text_input(\"What brings you here? \")\n",
        "  if user_question:\n",
        "    handle_userinput(user_question)\n",
        "\n",
        "  with st.sidebar:\n",
        "    st.subheader(\"Your documents\")\n",
        "    pdf_docs = st.file_uploader(\n",
        "        \"Upload your pdfs here and click on 'Process'\", accept_multiple_files = True)\n",
        "    if st.button(\"Process\"):\n",
        "      with st.spinner(\"Process\"):\n",
        "        # Perform loading on the pdf\n",
        "        text = get_pdf_text(pdf_docs)\n",
        "        # st.write(text)\n",
        "\n",
        "        # Perform text-splitting\n",
        "        docs  = get_text_chunks(text)\n",
        "\n",
        "        # Performing embeddings/vectorization using HuggingFaceEmbeddings\n",
        "        # and I'll be storing these embeddings in Chroma vector store\n",
        "        db = get_vectorstore(docs)\n",
        "\n",
        "        # Making a conversation chain\n",
        "        # conversation = get_conversation_chain(db)\n",
        "        # do the below one in case you want a persistent convo\n",
        "        st.session_state.conversation = get_conversation_chain(db)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3SBL3Zxr7P_I",
        "outputId": "ad4fb9a9-7aed-457c-cb33-8ba05189ce13"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run /content/app.py &>/content/logs.txt &"
      ],
      "metadata": {
        "id": "3dPt4bdh_p9Q"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!npx localtunnel --port 8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yp06VMqR_r76",
        "outputId": "337208ac-aa9a-437d-a5a9-9ed7c4ae05b0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25hnpx: installed 22 in 2.124s\n",
            "your url is: https://dark-flies-drum.loca.lt\n",
            "^C\n"
          ]
        }
      ]
    }
  ]
}